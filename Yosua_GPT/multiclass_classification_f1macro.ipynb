{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification Model with F1-Macro Average as Evaluation Metric\n",
    "\n",
    "In this notebook, we will create a multiclass classification model and use F1-Macro Average as the evaluation metric. F1-Macro Average treats all classes equally, providing a more balanced performance metric when you care about all classes equally.\n",
    "\n",
    "## Steps in this Notebook:\n",
    "1. Create a mock dataset for a multiclass classification problem.\n",
    "2. Train a classifier on the dataset.\n",
    "3. Evaluate the model using F1-Macro Average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a Mock Dataset\n",
    "We'll create a simple mock dataset with three classes and a few features for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock dataset creation\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(200, 5)  # 200 samples, 5 features\n",
    "y = np.random.choice([0, 1, 2], size=200)  # 3 classes: 0, 1, 2\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display dataset shapes\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train a Classifier\n",
    "We'll use a Random Forest Classifier for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate the Model with F1-Macro Average\n",
    "We'll compute the F1-Macro Average for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1-Macro Average\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "print('F1-Macro Average:', f1_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Detailed Classification Report\n",
    "In addition to the F1-Macro average, let's also print the full classification report for more insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The F1-Macro average is an important metric for evaluating models in multiclass classification when all classes are equally important. By using this metric, we can ensure that the model performs well across all classes and does not focus on just the majority class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
