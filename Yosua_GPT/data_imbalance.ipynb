{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Resampling Algorithms for Imbalanced Data\n",
    "\n",
    "## Introduction\n",
    "Imbalanced datasets can significantly affect the performance of machine learning models. This notebook explores various resampling techniques, including oversampling and undersampling, to balance the classes in a dataset. We will demonstrate the use of popular libraries such as `imbalanced-learn` along with `pandas` and `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we need to install the required libraries and import them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install imbalanced-learn if not already installed\n",
    "!pip install -U imbalanced-learn\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Set style for plots\n",
    "sns.set(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Synthetic Imbalanced Dataset\n",
    "We'll create a synthetic dataset with an imbalanced class distribution using `make_classification`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, n_samples=1000, weights=[0.9, 0.1],\n",
    "                           random_state=42)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "data['target'] = y\n",
    "\n",
    "# Display the class distribution\n",
    "class_counts = data['target'].value_counts()\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.title('Class Distribution Before Resampling')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Techniques\n",
    "We will explore three resampling techniques: SMOTE (Synthetic Minority Over-sampling Technique), Random Under-sampling, and SMOTE followed by ENN (Edited Nearest Neighbors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SMOTE\n",
    "SMOTE generates synthetic samples for the minority class to balance the class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the class distribution after SMOTE\n",
    "resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "sns.barplot(x=resampled_class_counts.index, y=resampled_class_counts.values)\n",
    "plt.title('Class Distribution After SMOTE')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_smote = RandomForestClassifier(random_state=42)\n",
    "rf_smote.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_smote = rf_smote.predict(X_test)\n",
    "print('Classification Report for SMOTE:\n",
    "', classification_report(y_test, y_pred_smote))\n",
    "print('Confusion Matrix for SMOTE:\n",
    "', confusion_matrix(y_test, y_pred_smote))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Under-sampling\n",
    "Random under-sampling reduces the number of samples from the majority class to balance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Random Under-sampling\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled_under, y_resampled_under = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the class distribution after under-sampling\n",
    "resampled_under_class_counts = pd.Series(y_resampled_under).value_counts()\n",
    "sns.barplot(x=resampled_under_class_counts.index, y=resampled_under_class_counts.values)\n",
    "plt.title('Class Distribution After Random Under-sampling')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_under = RandomForestClassifier(random_state=42)\n",
    "rf_under.fit(X_resampled_under, y_resampled_under)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_under = rf_under.predict(X_test)\n",
    "print('Classification Report for Random Under-sampling:\n",
    "', classification_report(y_test, y_pred_under))\n",
    "print('Confusion Matrix for Random Under-sampling:\n",
    "', confusion_matrix(y_test, y_pred_under))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SMOTE + Edited Nearest Neighbors (ENN)\n",
    "SMOTE followed by ENN generates synthetic samples and then removes samples from the majority class that are incorrectly classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE + ENN\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled_enn, y_resampled_enn = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the class distribution after SMOTE + ENN\n",
    "resampled_enn_class_counts = pd.Series(y_resampled_enn).value_counts()\n",
    "sns.barplot(x=resampled_enn_class_counts.index, y=resampled_enn_class_counts.values)\n",
    "plt.title('Class Distribution After SMOTE + ENN')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_enn = RandomForestClassifier(random_state=42)\n",
    "rf_enn.fit(X_resampled_enn, y_resampled_enn)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_enn = rf_enn.predict(X_test)\n",
    "print('Classification Report for SMOTE + ENN:\n",
    "', classification_report(y_test, y_pred_enn))\n",
    "print('Confusion Matrix for SMOTE + ENN:\n",
    "', confusion_matrix(y_test, y_pred_enn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we explored different data resampling techniques for handling imbalanced datasets, including SMOTE, Random Under-sampling, and SMOTE followed by ENN. Each method has its strengths and can improve the performance of machine learning models in different scenarios. \n",
    "\n",
    "Make sure to choose the resampling technique based on the specific context and requirements of your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
