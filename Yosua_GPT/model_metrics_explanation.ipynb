{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db839a5e",
   "metadata": {},
   "source": [
    "# Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88911e52",
   "metadata": {},
   "source": [
    "\n",
    "Model metrics are used to evaluate the performance of machine learning models. They provide quantitative values that help us understand how well a model is doing with respect to its tasks. Some commonly used metrics are:\n",
    "- **Accuracy**\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1 Score**\n",
    "- **AUC-ROC**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8736775",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70377ab5",
   "metadata": {},
   "source": [
    "\n",
    "Accuracy is the ratio of correctly predicted instances to the total instances. It's a good measure when the classes are balanced.\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "Where:\n",
    "- **TP**: True Positives\n",
    "- **TN**: True Negatives\n",
    "- **FP**: False Positives\n",
    "- **FN**: False Negatives\n",
    "\n",
    "**Example**: If a model predicts 80 out of 100 instances correctly, the accuracy is 80%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ee539",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c563637",
   "metadata": {},
   "source": [
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. It is a good metric when the cost of false positives is high.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "**Example**: If a spam detection model predicts 90 spam emails, out of which 80 are actually spam, the precision is $\\frac{80}{90} = 0.89$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0239d59",
   "metadata": {},
   "source": [
    "## Recall (Sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff39204",
   "metadata": {},
   "source": [
    "\n",
    "Recall is the ratio of correctly predicted positive observations to all observations in the actual class. Itâ€™s useful when the cost of false negatives is high.\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "**Example**: If out of 100 spam emails, a model correctly detects 80, the recall is $\\frac{80}{100} = 0.80$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706382e",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfeccb",
   "metadata": {},
   "source": [
    "\n",
    "F1 Score is the weighted average of precision and recall. It provides a balance between precision and recall and is useful when the classes are imbalanced.\n",
    "\n",
    "$$ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $$\n",
    "\n",
    "**Example**: If precision is 0.89 and recall is 0.80, the F1 score is:\n",
    "\n",
    "$$ F1 = 2 \\times \\frac{0.89 \\times 0.80}{0.89 + 0.80} = 0.845 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc61526",
   "metadata": {},
   "source": [
    "## AUC-ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0785963",
   "metadata": {},
   "source": [
    "\n",
    "AUC-ROC (Area Under the Receiver Operating Characteristics curve) measures the ability of the model to distinguish between classes. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR).\n",
    "\n",
    "- AUC = 1 means the model is perfect.\n",
    "- AUC = 0.5 means the model is no better than random guessing.\n",
    "\n",
    "AUC-ROC is especially useful for binary classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523ae7a",
   "metadata": {},
   "source": [
    "## Example Code for Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Example data\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "# ROC-AUC (assuming y_pred are probabilities in practice)\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "accuracy, precision, recall, f1, roc_auc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyh311",
   "language": "python",
   "name": "pyh311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
