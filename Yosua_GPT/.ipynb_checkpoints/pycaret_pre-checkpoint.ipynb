{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Preprocessing with PyCaret\n",
                "\n",
                "Preprocessing is a crucial step in any machine learning pipeline. PyCaret automates many preprocessing tasks such as handling missing data, encoding categorical variables, scaling numerical features, feature selection, and more. This notebook demonstrates how to use PyCaret for efficient data preprocessing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Installing PyCaret"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install PyCaret\n",
                "!pip install pycaret"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Importing Necessary Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pycaret.classification import *  # We can use this for classification or regression depending on the task\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Loading the Dataset\n",
                "\n",
                "We will load a dataset for demonstration purposes. Ensure that your dataset is in the form of a pandas DataFrame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load your dataset\n",
                "data = pd.read_csv('path_to_your_dataset.csv')\n",
                "\n",
                "# Display first few rows of the dataset\n",
                "data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Setting Up PyCaret for Preprocessing\n",
                "\n",
                "The `setup()` function in PyCaret initializes the preprocessing pipeline. Here, we specify various options for handling missing values, encoding, scaling, etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setting up the environment\n",
                "clf = setup(data=data, target='target_column_name', \n",
                "           session_id=123, \n",
                "           normalize=True,  # Normalize numeric features\n",
                "           transformation=True,  # Apply transformation (e.g., power transform)\n",
                "           handle_unknown_categorical=True,  # Handle unknown categories in new data\n",
                "           remove_multicollinearity=True,  # Remove highly correlated features\n",
                "           multicollinearity_threshold=0.9,  # Threshold for multicollinearity\n",
                "           ignore_low_variance=True,  # Remove features with low variance\n",
                "           feature_selection=True,  # Enable automatic feature selection\n",
                "           feature_interaction=True,  # Create interactions between features\n",
                "           pca=True,  # Apply PCA for dimensionality reduction\n",
                "           pca_components=0.95)  # Keep 95% of variance during PCA"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation of Setup Parameters:\n",
                "- `target`: The target column for your classification or regression task.\n",
                "- `normalize`: Normalizes numeric data to bring all features on the same scale.\n",
                "- `transformation`: Applies transformation to stabilize variance in data (e.g., power or log transformation).\n",
                "- `handle_unknown_categorical`: Helps deal with unknown categories in test data.\n",
                "- `remove_multicollinearity`: Automatically removes features that are highly correlated (multicollinear).\n",
                "- `ignore_low_variance`: Removes features with very low variance (constant features).\n",
                "- `feature_selection`: Automatically selects important features.\n",
                "- `feature_interaction`: Creates interaction terms between existing features.\n",
                "- `pca`: Applies Principal Component Analysis (PCA) for dimensionality reduction."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Handling Missing Values\n",
                "\n",
                "PyCaret automatically handles missing values using several imputation techniques."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the imputation strategy applied by PyCaret\n",
                "get_config('X').isnull().sum()  # Check if missing values were handled"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Encoding Categorical Variables\n",
                "\n",
                "PyCaret automatically encodes categorical variables using different encoding methods based on the data. You can control encoding via the `categorical_features` and `ordinal_features` parameters in `setup()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the transformed dataset with encoded categorical variables\n",
                "get_config('X').head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Scaling and Normalizing Data\n",
                "\n",
                "PyCaret provides options for scaling and normalizing the data to bring all features to a similar range. This is particularly useful when algorithms like SVM or K-Nearest Neighbors are being used."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scaling and normalization applied\n",
                "# You can view the first few rows of normalized data:\n",
                "get_config('X').head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Removing Multicollinearity\n",
                "\n",
                "Highly correlated features can negatively impact model performance. PyCaret automatically removes multicollinear features based on a specified threshold."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the features removed due to multicollinearity\n",
                "get_config('remove_multicollinearity')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Feature Selection\n",
                "\n",
                "PyCaret can automatically select the most important features in the dataset using various feature selection techniques. This reduces the dimensionality of the dataset and often improves model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check the selected features after feature selection\n",
                "selected_features = get_config('X').columns\n",
                "selected_features"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Feature Interaction\n",
                "\n",
                "PyCaret can automatically generate interaction terms between features, which may improve the modelâ€™s ability to capture complex relationships."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check feature interaction terms created by PyCaret\n",
                "get_config('X').head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Dimensionality Reduction with PCA\n",
                "\n",
                "Principal Component Analysis (PCA) is applied to reduce the number of features while retaining most of the variance. PyCaret automatically handles this by keeping 95% of the explained variance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check the components after PCA\n",
                "pca_components = get_config('X').head()\n",
                "pca_components"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Final Dataset for Modeling\n",
                "\n",
                "The preprocessed dataset is now ready for model training or further analysis. All transformations and feature engineering steps have been applied."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final preprocessed dataset ready for modeling\n",
                "get_config('X').head()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
