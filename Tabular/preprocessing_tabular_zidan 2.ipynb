{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "import pycaret\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # For scaling\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder  # For encoding categorical data\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Lokal: CSV, Excel, JSON, ZIP.\n",
    "#### Database: SQL.\n",
    "#### Big Data: Parquet, Feather, HDF5.\n",
    "#### Online Resources: URL, API, Google Sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('your_dataset.csv')\n",
    "df.head()  # Preview the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from an Excel file\n",
    "df = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a JSON file\n",
    "df = pd.read_json('data.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('database.db')\n",
    "\n",
    "# Load data from an SQL query\n",
    "df = pd.read_sql_query('SELECT * FROM tablename', conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a compressed ZIP file\n",
    "df = pd.read_csv('data.zip', compression='zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a URL\n",
    "df = pd.read_csv('https://url_to_data.com/data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Google Sheets (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Authenticate using Google API\n",
    "scope = ['https://spreadsheets.google.com/feeds']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open a sheet by name\n",
    "sheet = client.open('SheetName').sheet1\n",
    "\n",
    "# Get data as a list of lists\n",
    "data = sheet.get_all_records()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. API (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load data from a REST API\n",
    "response = requests.get('https://api.example.com/data')\n",
    "data = response.json()\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Parquet (HDFS/S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Parquet file\n",
    "df = pd.read_parquet('data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a Feather file\n",
    "df = pd.read_feather('data.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K. HDF5 (Hierarchical Data Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from an HDF5 file\n",
    "df = pd.read_hdf('data.h5', 'key_name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L. Pickle (Python Serializing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a Pickle file\n",
    "df = pd.read_pickle('data.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M. arff (Attribute-Relation File Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff('data.arff')\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penghapusan Baris/Kolom: Jika nilai hilang minor.\n",
    "#### Pengisian Nilai: Menggunakan konstanta, statistik deskriptif, interpolasi, atau model.\n",
    "#### Model Pembelajaran Mesin: Menggunakan KNN, regresi, atau algoritma khusus untuk mengisi nilai hilang.\n",
    "#### Khusus Kategori atau Data Waktu: Mengisi berdasarkan kategori atau metode forward/backward fill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Detect Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()  # Check how many missing values are in each column\n",
    "persentase_hilang = df.isnull().sum() / df.shape[0] * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete Row with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Menghapus baris dengan nilai hilang\n",
    "df = df.dropna()\n",
    "\n",
    "# Menghapus kolom dengan nilai hilang\n",
    "df = df.dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Fill Missing Values (Constanta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dengan nol\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Mengisi nilai hilang dengan nilai konstan lain\n",
    "df = df.fillna(value={'column1': 0, 'column2': 'unknown'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Fill Missing Values (Mean, Median, Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dengan rata-rata kolom\n",
    "df['column'] = df['column'].fillna(df['column'].mean())\n",
    "\n",
    "# Mengisi nilai hilang dengan median kolom\n",
    "df['column'] = df['column'].fillna(df['column'].median())\n",
    "\n",
    "# Mengisi nilai hilang dengan modus kolom\n",
    "df['column'] = df['column'].fillna(df['column'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Fill Missing Values (Category Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dengan modus kategori\n",
    "df['column'] = df.groupby('category')['column'].transform(lambda x: x.fillna(x.mode()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Fill Missing Values (Pembobotan dan Penyesuaian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dalam data waktu dengan pembobotan\n",
    "df['column'] = df['column'].fillna(method='ffill')  # Forward fill\n",
    "df['column'] = df['column'].fillna(method='bfill')  # Backward fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Fill Missing Values (Interpolasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dengan interpolasi linear\n",
    "df = df.interpolate()\n",
    "\n",
    "# Mengisi nilai hilang dengan interpolasi polinomial\n",
    "df = df.interpolate(method='polynomial', order=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Fill Missing Values (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Menggunakan KNN untuk imputasi\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Fill Missing Values (Regression Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Memisahkan data dengan nilai hilang\n",
    "df_missing = df[df['column'].isna()]\n",
    "df_complete = df.dropna(subset=['column'])\n",
    "\n",
    "# Melatih model regresi\n",
    "X = df_complete.drop(columns=['column'])\n",
    "y = df_complete['column']\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Mengisi nilai hilang dengan prediksi model\n",
    "X_missing = df_missing.drop(columns=['column'])\n",
    "df_missing['column'] = model.predict(X_missing)\n",
    "df = pd.concat([df_complete, df_missing])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K. Fill Missing Values (IterativeImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Menggunakan IterativeImputer\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L. Fill Missing Values (Imputation Model Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numerical data with the median\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df['num_column'] = num_imputer.fit_transform(df[['num_column']])\n",
    "\n",
    "# Fill missing categorical data with the most frequent value\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['cat_column'] = cat_imputer.fit_transform(df[['cat_column']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding: Digunakan untuk data kategorikal tanpa urutan.\n",
    "#### One-Hot Encoding: Untuk model yang tidak mendukung data kategorikal.\n",
    "#### Binary Encoding dan BaseN Encoding: Untuk mengurangi dimensi dibandingkan one-hot encoding.\n",
    "#### Frequency dan Count Encoding: Menggunakan frekuensi kategori.\n",
    "#### Target Encoding: Menggunakan rata-rata target per kategori.\n",
    "#### Ordinal Encoding: Untuk data kategorikal yang memiliki urutan.\n",
    "#### Hash Encoding dan Leave-One-Out Encoding: Untuk data besar atau data dengan banyak kategori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Label Encoding (For Ordinal Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['category_encoded'] = label_encoder.fit_transform(df['category_column'])\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for col in df_baru.columns:\n",
    "    if df_baru[col].dtype == 'object':\n",
    "        df_baru[col] = label_encoder.fit_transform(df_baru[col])\n",
    "        df_baru[col] = df_baru[col].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. One-Hot Encoding (For Nominal Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['category_column'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "# Inisialisasi BinaryEncoder\n",
    "encoder = ce.BinaryEncoder(cols=['categorical_column'])\n",
    "\n",
    "# Menggunakan BinaryEncoder\n",
    "df_encoded = encoder.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung frekuensi\n",
    "frequency = df['categorical_column'].value_counts()\n",
    "\n",
    "# Mengganti kategori dengan frekuensi\n",
    "df['encoded_column'] = df['categorical_column'].map(frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Target Encoding (Mean Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung rata-rata target per kategori\n",
    "mean_encoding = df.groupby('categorical_column')['target'].mean()\n",
    "\n",
    "# Mengganti kategori dengan rata-rata target\n",
    "df['encoded_column'] = df['categorical_column'].map(mean_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Inisialisasi OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Menggunakan OrdinalEncoder pada kolom\n",
    "df['encoded_column'] = encoder.fit_transform(df[['categorical_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. BaseN Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Inisialisasi BaseNEncoder dengan basis 3\n",
    "encoder = ce.BaseNEncoder(base=3, cols=['categorical_column'])\n",
    "\n",
    "# Menggunakan BaseNEncoder\n",
    "df_encoded = encoder.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Count Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung jumlah kemunculan\n",
    "count_encoding = df['categorical_column'].value_counts()\n",
    "\n",
    "# Mengganti kategori dengan jumlah kemunculan\n",
    "df['encoded_column'] = df['categorical_column'].map(count_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Hash Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Inisialisasi HashingEncoder dengan n_components\n",
    "encoder = ce.HashingEncoder(n_components=8, cols=['categorical_column'])\n",
    "\n",
    "# Menggunakan HashingEncoder\n",
    "df_encoded = encoder.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Leave-One-Out Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Inisialisasi HashingEncoder dengan n_components\n",
    "encoder = ce.HashingEncoder(n_components=8, cols=['categorical_column'])\n",
    "\n",
    "# Menggunakan HashingEncoder\n",
    "df_encoded = encoder.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization: Menyediakan fitur dengan distribusi normal.\n",
    "#### Min-Max Scaling: Mengubah rentang data ke [0, 1].\n",
    "#### MaxAbs Scaling: Berguna untuk data dengan nilai positif dan negatif.\n",
    "#### Robust Scaling: Mengurangi dampak outlier.\n",
    "#### Quantile Transformation: Mengubah distribusi fitur.\n",
    "#### Power Transformation: Membantu stabilisasi varians.\n",
    "#### Normalizer: Menormalkan fitur ke norma unit.\n",
    "#### Custom Scaling: Menyediakan kontrol penuh untuk scaling.\n",
    "#### Binarization: Mengubah fitur ke format biner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Standardization\n",
    "Standardization (also called Z-score normalization) transforms the data to have a mean of 0 and a standard deviation of 1. This method is particularly useful when the data follows a Gaussian distribution (normal distribution).\n",
    "\n",
    "\n",
    "2. Normalization\n",
    "Normalization (also known as Min-Max scaling) rescales the feature to a fixed range, typically [0, 1]. This method is useful when you want to ensure that all features have the same scale, especially when using algorithms that are sensitive to the scale of input data (e.g., neural networks).\n",
    "\n",
    "Key Differences\n",
    "Purpose: Standardization centers the data around zero, while normalization rescales the data to a specific range.\n",
    "Effect on Data: Standardization retains the distribution shape, while normalization compresses all values into a defined range.\n",
    "Use Cases: Standardization is preferred for algorithms that assume Gaussian distributions, whereas normalization is useful for algorithms that rely on distance measures, like k-nearest neighbors or gradient descent optimization in neural networks.\n",
    "\n",
    "Conclusion\n",
    "Both standardization and normalization are essential preprocessing steps that can significantly affect the performance of machine learning models. The choice between them depends on the characteristics of your data and the requirements of the specific algorithm you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Standardization (Z-score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['num_column1', 'num_column2']] = scaler.fit_transform(df[['num_column1', 'num_column2']])\n",
    "\n",
    "#Import: Kita mengimpor pandas untuk membuat dan mengelola DataFrame, dan StandardScaler dari sklearn.preprocessing untuk melakukan standardisasi.\n",
    "#DataFrame: Kita membuat DataFrame dengan dua kolom numerik (num_column1 dan num_column2) serta satu kolom kategori.\n",
    "#StandardScaler: Kita membuat objek StandardScaler untuk menstandardisasi data.\n",
    "#fit_transform: Fungsi ini digunakan untuk menghitung mean dan standard deviation dari kolom yang diberikan, dan kemudian menerapkan transformasi standardisasi pada data.\n",
    "\n",
    "#Kegunaan StandardScaler\n",
    "#Menghilangkan Skala: StandardScaler mengubah data sehingga memiliki mean 0 dan standar deviasi 1. Ini sangat berguna ketika fitur memiliki skala yang berbeda dan dapat mempengaruhi hasil model.\n",
    "\n",
    "#Meningkatkan Kinerja Model: Banyak algoritma, seperti K-Nearest Neighbors (KNN) dan Support Vector Machines (SVM), sangat sensitif terhadap skala data. Standardisasi dapat meningkatkan kinerja model dengan memberikan kontribusi yang lebih seimbang dari setiap fitur.\n",
    "\n",
    "#Meningkatkan Konvergensi: Dalam algoritma optimasi, seperti Gradient Descent yang digunakan dalam pembelajaran mendalam, standardisasi dapat mempercepat konvergensi karena semua fitur berada pada skala yang sama.\n",
    "\n",
    "#Memudahkan Interpretasi: Dengan memiliki fitur yang terstandardisasi, hasil analisis atau model dapat lebih mudah diinterpretasikan karena tidak terpengaruh oleh skala yang berbeda antara fitur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "df[['num_column1', 'num_column2']] = min_max_scaler.fit_transform(df[['num_column1', 'num_column2']])\n",
    "\n",
    "#Import: Kita mengimpor pandas untuk membuat dan mengelola DataFrame, dan MinMaxScaler dari sklearn.preprocessing untuk melakukan normalisasi.\n",
    "#DataFrame: Kita membuat DataFrame dengan dua kolom numerik (num_column1 dan num_column2) serta satu kolom kategori.\n",
    "#MinMaxScaler: Kita membuat objek MinMaxScaler untuk melakukan normalisasi pada data.\n",
    "#fit_transform: Fungsi ini menghitung nilai minimum dan maksimum dari kolom yang diberikan, dan kemudian menerapkan transformasi normalisasi sehingga semua nilai berada dalam rentang [0, 1].\n",
    "\n",
    "\n",
    "#Kegunaan MinMaxScaler\n",
    "#Skala Data Konsisten: Normalisasi dengan MinMaxScaler memastikan bahwa semua fitur berada dalam rentang yang sama, biasanya [0, 1]. Ini membantu algoritma yang bergantung pada jarak, seperti K-Nearest Neighbors (KNN) dan Support Vector Machines (SVM).\n",
    "\n",
    "#Menghindari Dominasi Fitur: Ketika fitur memiliki skala yang berbeda, fitur dengan skala yang lebih besar dapat mendominasi hasil model. Normalisasi membantu menghindari masalah ini dengan memberikan kontribusi yang seimbang dari setiap fitur.\n",
    "\n",
    "#Meningkatkan Kinerja Model: Dengan fitur yang dinormalisasi, algoritma pembelajaran mesin dapat beroperasi lebih efektif, meningkatkan akurasi dan kecepatan konvergensi.\n",
    "\n",
    "#Mempermudah Interpretasi: Hasil model menjadi lebih mudah diinterpretasikan ketika semua fitur berada dalam rentang yang sama, sehingga analisis lebih intuitif.\n",
    "\n",
    "#Bermanfaat untuk Data yang Tidak Terdistribusi Normal: Jika data tidak mengikuti distribusi normal, normalisasi dengan MinMaxScaler dapat menjadi pilihan yang lebih baik dibandingkan standardisasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. MaxAbs Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Inisialisasi MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n",
    "\n",
    "\n",
    "#Import: Kita mengimpor pandas untuk membuat dan mengelola DataFrame, serta MaxAbsScaler dari sklearn.preprocessing untuk melakukan normalisasi.\n",
    "#DataFrame: Kita membuat DataFrame dengan satu kolom numerik (feature_column) yang memiliki nilai positif dan negatif.\n",
    "#MaxAbsScaler: Kita membuat objek MaxAbsScaler untuk melakukan normalisasi berdasarkan nilai maksimum absolut.\n",
    "#fit_transform: Fungsi ini menghitung nilai maksimum absolut dari kolom yang diberikan, dan kemudian menerapkan transformasi sehingga semua nilai berada dalam rentang [-1, 1].\n",
    "\n",
    "\n",
    "#Kegunaan MaxAbsScaler\n",
    "#Mempertahankan Tanda: MaxAbsScaler menjaga tanda asli dari nilai (positif/negatif). Ini sangat berguna ketika data memiliki arah yang signifikan dan Anda ingin mempertahankan informasi tersebut.\n",
    "\n",
    "#Skala Data yang Konsisten: Dengan menggunakan MaxAbsScaler, semua fitur dinormalisasi dalam rentang [-1, 1], sehingga memudahkan komparasi antar fitur.\n",
    "\n",
    "#Cocok untuk Data Spars: MaxAbsScaler sangat bermanfaat untuk dataset yang memiliki banyak nilai nol (sparse data), seperti yang sering ditemui dalam pengolahan teks dan data kategori. Normalisasi ini tidak mengubah nilai nol dan tetap mempertahankan bentuk data asli.\n",
    "\n",
    "#Meningkatkan Kinerja Model: Dengan fitur yang dinormalisasi, algoritma pembelajaran mesin dapat beroperasi lebih efektif, terutama algoritma yang sensitif terhadap skala data, seperti SVM atau neural networks.\n",
    "\n",
    "#Meningkatkan Interpretasi: Normalisasi membuat interpretasi hasil analisis lebih mudah, karena semua fitur sekarang berada dalam skala yang sama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Robust Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Inisialisasi RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n",
    "\n",
    "\n",
    "#Import: Kita mengimpor pandas untuk membuat dan mengelola DataFrame, serta RobustScaler dari sklearn.preprocessing untuk melakukan normalisasi.\n",
    "#DataFrame: Kita membuat DataFrame dengan satu kolom numerik (feature_column) yang berisi beberapa nilai yang tinggi (outlier) dan nilai normal.\n",
    "#RobustScaler: Kita membuat objek RobustScaler untuk melakukan normalisasi yang tahan terhadap outlier.\n",
    "#fit_transform: Fungsi ini menghitung median dan kuartil dari kolom yang diberikan, lalu menerapkan transformasi normalisasi berdasarkan median dan rentang interkuartil (IQR).\n",
    "\n",
    "\n",
    "#Kegunaan RobustScaler\n",
    "#Tahan Terhadap Outlier: RobustScaler menggunakan median dan interquartile range (IQR) untuk normalisasi, yang membuatnya kurang sensitif terhadap outlier dibandingkan metode lain seperti Min-Max Scaling atau Standardization.\n",
    "\n",
    "#Mempertahankan Distribusi Data: Dengan menggunakan median dan IQR, RobustScaler mempertahankan distribusi data asli, sehingga memberikan hasil yang lebih representatif ketika data mengandung outlier.\n",
    "\n",
    "#Skala Data Konsisten: Normalisasi dengan RobustScaler menghasilkan skala yang konsisten, dengan nilai yang biasanya berada dalam rentang [-1, 1], yang memudahkan interpretasi dan komparasi.\n",
    "\n",
    "#Cocok untuk Data dengan Distribusi Asimetris: RobustScaler sangat berguna ketika data tidak terdistribusi normal dan memiliki banyak outlier, sehingga dapat digunakan dalam berbagai situasi.\n",
    "\n",
    "#Meningkatkan Kinerja Model: Dengan fitur yang dinormalisasi dengan cara yang tahan terhadap outlier, algoritma pembelajaran mesin dapat beroperasi lebih efektif, meningkatkan akurasi model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Quantile Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Inisialisasi QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n",
    "\n",
    "\n",
    "#Import: Kita mengimpor pandas untuk membuat dan mengelola DataFrame, serta QuantileTransformer dari sklearn.preprocessing untuk melakukan normalisasi.\n",
    "#DataFrame: Kita membuat DataFrame dengan satu kolom numerik (feature_column) yang berisi beberapa nilai, termasuk outlier.\n",
    "#QuantileTransformer: Kita membuat objek QuantileTransformer dengan parameter output_distribution='uniform' untuk mendistribusikan nilai ke dalam rentang [0, 1].\n",
    "#fit_transform: Fungsi ini menghitung kuantil dari kolom yang diberikan, lalu menerapkan transformasi sehingga semua nilai didistribusikan secara merata dalam rentang [0, 1].\n",
    "\n",
    "\n",
    "#Kegunaan QuantileTransformer\n",
    "#Transformasi ke Distribusi Uniform: QuantileTransformer mendistribusikan nilai-nilai ke dalam distribusi uniform, yang membuatnya cocok untuk algoritma yang memerlukan data dalam skala tertentu.\n",
    "\n",
    "#Tahan Terhadap Outlier: Karena menggunakan kuantil, QuantileTransformer kurang sensitif terhadap outlier dibandingkan metode normalisasi tradisional. Ini membuatnya ideal untuk data yang memiliki nilai ekstrem.\n",
    "\n",
    "#Cocok untuk Data Tidak Terdistribusi Normal: Metode ini dapat digunakan pada data yang tidak mengikuti distribusi normal, membantu untuk mempersiapkan data dengan cara yang lebih sesuai untuk analisis atau pemodelan lebih lanjut.\n",
    "\n",
    "#Meningkatkan Kinerja Model: Dengan mendistribusikan nilai secara merata, QuantileTransformer dapat meningkatkan kinerja model machine learning, terutama pada algoritma yang sensitif terhadap skala data.\n",
    "\n",
    "#Mempermudah Interpretasi: Normalisasi dengan QuantileTransformer membuat interpretasi hasil analisis lebih mudah, karena semua fitur berada dalam skala yang sama dan lebih terdistribusi secara merata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Power Transformation (Box-Cox & Yeo-Johnson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Inisialisasi PowerTransformer dengan metode Yeo-Johnson\n",
    "scaler = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n",
    "\n",
    "\n",
    "#Import: Kita mengimpor pandas untuk membuat dan mengelola DataFrame, serta PowerTransformer dari sklearn.preprocessing untuk melakukan transformasi.\n",
    "#DataFrame: Kita membuat DataFrame dengan satu kolom numerik (feature_column) yang berisi beberapa nilai, termasuk outlier.\n",
    "#PowerTransformer: Kita membuat objek PowerTransformer dengan parameter method='yeo-johnson', yang memungkinkan kita untuk menerapkan transformasi yang dapat menangani nilai nol dan negatif.\n",
    "#fit_transform: Fungsi ini menghitung parameter yang diperlukan (seperti lambda untuk transformasi) dan menerapkan transformasi pada data.\n",
    "\n",
    "\n",
    "#Kegunaan PowerTransformer\n",
    "#Mengatasi Asimetri Data: PowerTransformer, khususnya dengan metode Yeo-Johnson, dirancang untuk mengubah data yang tidak terdistribusi normal menjadi lebih mendekati distribusi normal. Ini sangat berguna untuk algoritma yang mengasumsikan distribusi normal.\n",
    "\n",
    "#Menangani Outlier: Metode ini juga dapat membantu mengurangi dampak dari outlier pada analisis dan pemodelan, menjadikan data lebih stabil.\n",
    "\n",
    "#Kesesuaian dengan Data Negatif: Berbeda dengan Box-Cox, Yeo-Johnson dapat digunakan pada data yang mengandung nilai negatif dan nol, sehingga lebih fleksibel dalam berbagai jenis dataset.\n",
    "\n",
    "#Meningkatkan Kinerja Model: Dengan mendekatkan distribusi data ke distribusi normal, PowerTransformer dapat meningkatkan kinerja model machine learning yang bergantung pada asumsi normalitas.\n",
    "\n",
    "#Mempermudah Interpretasi: Transformasi data sehingga lebih mendekati distribusi normal dapat membuat hasil analisis lebih mudah dipahami dan diinterpretasikan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Inisialisasi Normalizer dengan norma L2\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n",
    "\n",
    "\n",
    "#Import: Kita mengimpor pandas untuk membuat dan mengelola DataFrame, serta Normalizer dari sklearn.preprocessing untuk melakukan normalisasi.\n",
    "#DataFrame: Kita membuat DataFrame dengan satu kolom numerik (feature_column) yang berisi beberapa nilai.\n",
    "#Normalizer: Kita membuat objek Normalizer dengan parameter norm='l2', yang akan menormalkan data sehingga panjang vektornya (norma) menjadi 1.\n",
    "#fit_transform: Fungsi ini menghitung norma dari kolom yang diberikan dan menerapkan normalisasi berdasarkan norma L2, menghasilkan vektor yang memiliki panjang 1.\n",
    "\n",
    "\n",
    "#Kegunaan Normalizer\n",
    "#Normalisasi Vektor: Normalizer digunakan untuk menormalkan setiap sampel (baris) dalam dataset sehingga panjang vektornya menjadi 1. Ini penting dalam konteks di mana sudut antar vektor lebih penting daripada magnitudonya.\n",
    "\n",
    "#Cocok untuk Algoritma Berbasis Jarak: Normalisasi sangat berguna untuk algoritma seperti K-Nearest Neighbors (KNN) atau clustering, di mana jarak antara data merupakan faktor kunci dalam klasifikasi atau pengelompokan.\n",
    "\n",
    "#Meningkatkan Kinerja Model: Dengan menggunakan normalisasi, kita dapat mengurangi pengaruh fitur dengan skala besar yang dapat mendominasi hasil analisis, sehingga meningkatkan kinerja model.\n",
    "\n",
    "#Menjaga Informasi Arah: Normalization dengan norma L2 tidak mengubah arah data tetapi hanya menskalakan magnitudo, sehingga informasi tentang hubungan antara data tetap terjaga.\n",
    "\n",
    "#Menghindari Overfitting: Normalisasi juga dapat membantu mengurangi risiko overfitting dengan memastikan bahwa semua fitur memiliki pengaruh yang seimbang pada model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Unit Vector Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Inisialisasi Normalizer dengan norma L2\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n",
    "\n",
    "\n",
    "#Import: Mengimpor pandas untuk manipulasi DataFrame dan Normalizer dari sklearn.preprocessing untuk normalisasi.\n",
    "#DataFrame: Membuat DataFrame dengan kolom feature_column yang berisi beberapa nilai numerik.\n",
    "#Normalizer: Membuat objek Normalizer dengan parameter norm='l2', yang akan menormalkan setiap baris dalam DataFrame sehingga norma (panjang) vektornya menjadi 1.\n",
    "#fit_transform: Menghitung norma dari kolom yang diberikan dan menerapkan normalisasi, menghasilkan array yang memiliki panjang 1 untuk setiap baris.\n",
    "\n",
    "\n",
    "#Kegunaan Normalizer\n",
    "#Normalisasi Vektor: Normalizer mengubah setiap baris menjadi vektor dengan panjang 1, penting dalam konteks di mana sudut antar vektor lebih penting daripada magnitudonya.\n",
    "\n",
    "#Cocok untuk Algoritma Berbasis Jarak: Sangat berguna untuk algoritma seperti K-Nearest Neighbors (KNN) dan clustering, di mana jarak antar data sangat berpengaruh.\n",
    "\n",
    "#Meningkatkan Kinerja Model: Mengurangi pengaruh fitur dengan skala besar yang dapat mendominasi hasil analisis, meningkatkan kinerja model secara keseluruhan.\n",
    "\n",
    "#Mempertahankan Informasi Arah: Menggunakan norma L2 tidak mengubah arah data tetapi hanya menskalakan magnitudo, sehingga hubungan antara data tetap terjaga.\n",
    "\n",
    "#Mencegah Overfitting: Dengan normalisasi, risiko overfitting dapat berkurang karena semua fitur akan memiliki pengaruh yang lebih seimbang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Custom Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fungsi custom untuk scaling\n",
    "def custom_scaling(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "# Menerapkan custom scaling\n",
    "df['scaled_column'] = custom_scaling(df['feature_column'])\n",
    "\n",
    "#Import: Mengimpor pandas untuk manipulasi DataFrame dan numpy untuk operasi numerik.\n",
    "#DataFrame: Membuat DataFrame dengan kolom feature_column yang berisi beberapa nilai numerik.\n",
    "#Fungsi Custom Scaling:\n",
    "#Fungsi custom_scaling(x) menerima array x dan mengembalikan nilai yang telah diskalakan menggunakan rumus min-max.\n",
    "#Rumus: (𝑥−min(𝑥))/(max(𝑥)−min(𝑥))(x−min(x))/(max(x)−min(x)) mengubah setiap nilai dalam x menjadi rentang [0, 1].\n",
    "#Menerapkan Scaling: Fungsi custom_scaling diterapkan pada kolom feature_column, dan hasilnya disimpan dalam kolom baru scaled_column.\n",
    "\n",
    "#Kegunaan Custom Scaling\n",
    "#Kustomisasi Proses Scaling: Dengan menggunakan fungsi kustom, kita dapat menyesuaikan rumus atau metode scaling sesuai dengan kebutuhan spesifik.\n",
    "\n",
    "#Menangani Data dengan Cara Unik: Kita dapat melakukan transformasi yang tidak terbatas pada metode standar seperti Min-Max atau Z-score, misalnya, menyesuaikan skala berdasarkan kriteria tertentu.\n",
    "\n",
    "#Pendidikan dan Pemahaman: Membuat fungsi kustom membantu dalam memahami mekanisme scaling dan bagaimana pengaruhnya terhadap data, yang bermanfaat untuk pembelajaran dan eksplorasi.\n",
    "\n",
    "#Fleksibilitas: Dapat dengan mudah dimodifikasi untuk menerapkan teknik lain atau menggabungkan beberapa metode scaling dalam satu fungsi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K. Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Inisialisasi Binarizer dengan threshold\n",
    "scaler = Binarizer(threshold=0.5)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n",
    "\n",
    "\n",
    "#Import: Mengimpor pandas untuk manipulasi DataFrame dan Binarizer dari sklearn.preprocessing untuk binarisasi.\n",
    "#DataFrame: Membuat DataFrame dengan kolom feature_column yang berisi beberapa nilai numerik, termasuk nilai di bawah dan di atas threshold.\n",
    "#Binarizer: Membuat objek Binarizer dengan parameter threshold=0.5, yang akan mengubah semua nilai di atas 0.5 menjadi 1 dan yang di bawahnya menjadi 0.\n",
    "#fit_transform: Menerapkan binarisasi pada kolom yang diberikan, menghasilkan array yang berisi 0 atau 1.\n",
    "\n",
    "\n",
    "#Kegunaan Binarizer\n",
    "#Membuat Data Kategori: Binarizer sangat berguna untuk mengubah data kontinu menjadi kategori. Ini penting dalam konteks model klasifikasi, di mana fitur perlu dalam bentuk biner.\n",
    "\n",
    "#Meningkatkan Interpretabilitas: Mengubah nilai menjadi 0 dan 1 dapat memudahkan pemahaman hasil analisis dan membuat visualisasi lebih sederhana.\n",
    "\n",
    "#Memudahkan Penggunaan dalam Algoritma Klasifikasi: Banyak algoritma machine learning, seperti Logistic Regression, memerlukan fitur dalam bentuk biner untuk pemodelan.\n",
    "\n",
    "#Penanganan Data Besar: Binarisasi dapat membantu mereduksi kompleksitas data, terutama ketika kita hanya peduli pada kehadiran atau ketidakhadiran suatu fitur.\n",
    "\n",
    "#Fleksibilitas Threshold: Pengguna dapat menentukan threshold yang sesuai dengan konteks data, memungkinkan penyesuaian yang lebih baik terhadap analisis yang diinginkan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Splitting Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation: Menggunakan seluruh data untuk pelatihan dan pengujian, memberikan penilaian model yang lebih robust.\n",
    "#### Stratified K-Fold: Menjaga distribusi target dalam setiap fold.\n",
    "#### Time Series Split: Menjaga urutan waktu dalam data.\n",
    "#### LOOCV: Membagi data dengan satu data sebagai uji pada setiap iterasi.\n",
    "#### Group K-Fold: Mempertahankan kelompok tertentu dalam data.\n",
    "#### Shuffle Split: Membagi data secara acak dengan proporsi yang ditentukan.\n",
    "#### Custom Split: Pembagian data berdasarkan kriteria khusus.\n",
    "#### Bootstrap Sampling: Menggunakan sampling dengan pengembalian untuk data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Split Standar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Inisialisasi KFold dengan jumlah fold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Menggunakan KFold untuk membagi data\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Stratified K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Inisialisasi StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Menggunakan StratifiedKFold untuk membagi data\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Inisialisasi TimeSeriesSplit dengan jumlah fold\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Menggunakan TimeSeriesSplit untuk membagi data\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Leave-One-Out Cross-Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Inisialisasi LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Menggunakan LeaveOneOut untuk membagi data\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Group K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Inisialisasi GroupKFold dengan jumlah fold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Menggunakan GroupKFold untuk membagi data\n",
    "for train_index, test_index in gkf.split(X, y, groups=groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Shuffle Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Inisialisasi ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "# Menggunakan ShuffleSplit untuk membagi data\n",
    "for train_index, test_index in ss.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh custom split berdasarkan kriteria waktu\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Menggunakan resample untuk membuat subset\n",
    "X_train, X_test, y_train, y_test = resample(X, y, n_samples=0.8, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifikasi dan Penghapusan: Menggunakan metode statistik atau visual untuk mengidentifikasi dan menghapus outliers.\n",
    "#### Transformasi Data: Menggunakan transformasi matematis untuk mengurangi efek outliers.\n",
    "#### Robust Methods: Metode scaling dan winsorizing yang mengurangi dampak outliers.\n",
    "#### Imputasi Outliers: Mengganti atau interpolasi nilai outliers untuk menjaga konsistensi data.\n",
    "#### Algorithm-Based Methods: Menggunakan algoritma machine learning untuk mendeteksi dan menangani outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Using Z-score to Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "z_scores = np.abs(stats.zscore(df[['num_column1', 'num_column2']]))\n",
    "df_cleaned = df[(z_scores < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. IQR (Interquartile Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['feature_column'].quantile(0.25)\n",
    "Q3 = df['feature_column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Menentukan batas bawah dan atas\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "# Mengidentifikasi outliers\n",
    "outliers = (df['feature_column'] < lower_bound) | (df['feature_column'] > upper_bound)\n",
    "# Menghapus outliers\n",
    "df_no_outliers = df[~outliers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Visualisasi (Boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Membuat boxplot\n",
    "sns.boxplot(x=df['feature_column'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mengaplikasikan log transformation\n",
    "df['log_feature'] = np.log1p(df['feature_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Square Root Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengaplikasikan square root transformation\n",
    "df['sqrt_feature'] = np.sqrt(df['feature_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Robust Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Inisialisasi RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit dan transformasi data\n",
    "df[['scaled_feature']] = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Trimming/ Winsorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats\n",
    "\n",
    "# Winsorizing data\n",
    "df['winsorized_feature'] = mstats.winsorize(df['feature_column'], limits=[0.05, 0.05])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Imputasi dengan Nilai Median atau Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengganti outliers dengan median\n",
    "median_value = df['feature_column'].median()\n",
    "df['feature_column'] = np.where(outliers, median_value, df['feature_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Imputasi dengan Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolasi data\n",
    "df['interpolated_feature'] = df['feature_column'].interpolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Inisialisasi IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.01)\n",
    "\n",
    "# Fit model\n",
    "outliers = iso_forest.fit_predict(df[['feature_column']])\n",
    "# Mengidentifikasi outliers\n",
    "df_no_outliers = df[outliers == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K. One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Inisialisasi OneClassSVM\n",
    "ocsvm = OneClassSVM(gamma='auto', nu=0.01)\n",
    "\n",
    "# Fit model\n",
    "outliers = ocsvm.fit_predict(df[['feature_column']])\n",
    "# Mengidentifikasi outliers\n",
    "df_no_outliers = df[outliers == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-Based Feature Importance: Menggunakan output model untuk menilai pentingnya fitur.\n",
    "#### Recursive Feature Elimination (RFE): Metode iteratif untuk menghapus fitur yang tidak penting.\n",
    "#### Mutual Information: Mengukur ketergantungan antara fitur dan target.\n",
    "#### Correlation Matrix: Mengidentifikasi fitur yang berkorelasi dengan target atau antar fitur.\n",
    "#### L1 Regularization (Lasso): Menggunakan regularisasi untuk memilih fitur penting.\n",
    "#### Genetic Algorithms: Metode berbasis evolusi untuk memilih fitur optimal.\n",
    "#### Feature Selection using AutoML: Mengotomatisasi proses feature selection dan tuning.\n",
    "#### Unsupervised Feature Selection: Menggunakan teknik unsupervised untuk memilih fitur penting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Removing Low Variance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "df_high_variance = selector.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Model-Based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inisialisasi dan fit model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Mengurutkan fitur berdasarkan kepentingan\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in indices:\n",
    "    print(f\"Feature {i}: {importances[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Menghitung permutation feature importance\n",
    "results = permutation_importance(model, X_test, y_test, scoring='accuracy')\n",
    "importances = results.importances_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inisialisasi model dan RFE\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit RFE\n",
    "fit = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan fitur yang terpilih\n",
    "selected_features = fit.support_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Menghitung mutual information\n",
    "mi = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "# Mengurutkan fitur berdasarkan mutual information\n",
    "indices = np.argsort(mi)[::-1]\n",
    "for i in indices:\n",
    "    print(f\"Feature {i}: {mi[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. L1 Regularization (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Inisialisasi Lasso\n",
    "lasso = Lasso(alpha=0.01)\n",
    "\n",
    "# Fit model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan koefisien fitur\n",
    "coefficients = lasso.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Inisialisasi RFECV dengan Logistic Regression\n",
    "selector = RFECV(LogisticRegression(), step=1, cv=5)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan fitur yang terpilih\n",
    "selected_features = selector.support_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Feature Selection using AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Inisialisasi TPOT\n",
    "tpot = TPOTClassifier(verbosity=2)\n",
    "\n",
    "# Fit model\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan pipeline terbaik\n",
    "print(tpot.fitted_pipeline_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Unsupervised Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Inisialisasi PCA\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# Fit PCA\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Mendapatkan komponen utama\n",
    "components = pca.components_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Binning Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal Width Binning: Membagi rentang data menjadi bin dengan lebar yang sama.\n",
    "#### Equal Frequency Binning: Membagi data sehingga setiap bin memiliki jumlah data yang sama.\n",
    "#### Custom Binning: Menetapkan interval bin secara manual.\n",
    "#### K-Means Binning: Menggunakan clustering untuk menentukan bin.\n",
    "#### Decision Tree Binning: Menentukan bin berdasarkan model decision tree.\n",
    "#### Equal Width Binning with Variable Bin Width: Menggunakan bin dengan lebar bervariasi berdasarkan distribusi data.\n",
    "#### Binning with Aggregation: Menghitung statistik agregasi per bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Equal Width Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Mendefinisikan jumlah bin\n",
    "num_bins = 5\n",
    "\n",
    "# Menggunakan cut untuk binning\n",
    "df['binned_feature'] = pd.cut(df['feature'], bins=num_bins, labels=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Equal Frequency Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Mendefinisikan jumlah bin\n",
    "num_bins = 5\n",
    "\n",
    "# Menggunakan qcut untuk binning\n",
    "df['binned_feature'] = pd.qcut(df['feature'], q=num_bins, labels=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Custom Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Mendefinisikan interval bin\n",
    "bins = [-np.inf, -1, 0, 1, np.inf]\n",
    "labels = ['Very Low', 'Low', 'High', 'Very High']\n",
    "\n",
    "# Menggunakan cut untuk binning\n",
    "df['binned_feature'] = pd.cut(df['feature'], bins=bins, labels=labels)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. K-Means Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Menentukan jumlah cluster (bin)\n",
    "num_bins = 5\n",
    "\n",
    "# Inisialisasi dan fit KMeans\n",
    "kmeans = KMeans(n_clusters=num_bins, random_state=0).fit(df[['feature']])\n",
    "\n",
    "# Menambahkan label cluster ke dataframe\n",
    "df['binned_feature'] = kmeans.labels_\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Decision Tree Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100), 'target': np.random.randn(100)})\n",
    "\n",
    "# Inisialisasi dan fit DecisionTreeRegressor\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=5, random_state=0)\n",
    "tree.fit(df[['feature']], df['target'])\n",
    "\n",
    "# Mendapatkan batas bin\n",
    "bins = np.sort(tree.tree_.threshold[tree.tree_.threshold != -2])\n",
    "bins = np.append(bins, np.inf)  # Menambahkan batas atas\n",
    "\n",
    "# Menggunakan cut untuk binning\n",
    "df['binned_feature'] = pd.cut(df['feature'], bins=bins, include_lowest=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Equal Width Binning with Variable Bin Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Mendefinisikan lebar bin berdasarkan persentil\n",
    "percentiles = [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "bins = np.percentile(df['feature'], q=percentiles)\n",
    "\n",
    "# Menggunakan cut untuk binning\n",
    "df['binned_feature'] = pd.cut(df['feature'], bins=bins, labels=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Binning with Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Menggunakan qcut untuk binning\n",
    "df['binned_feature'] = pd.qcut(df['feature'], q=5, labels=False)\n",
    "\n",
    "# Menghitung statistik agregasi per bin\n",
    "bin_stats = df.groupby('binned_feature').agg({'feature': ['mean', 'std', 'count']})\n",
    "\n",
    "print(bin_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Data Balancing (Handling Imbalanced Classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling Methods: SMOTE, ADASYN, Random Oversampling.\n",
    "\n",
    "#### Kegunaan: Menambah data pada kelas minoritas.\n",
    "#### Catatan: Memerlukan penanganan overfitting dan mungkin memperbesar ukuran dataset.\n",
    "### Undersampling Methods: Random Undersampling, Tomek Links, Edited Nearest Neighbors.\n",
    "\n",
    "#### Kegunaan: Mengurangi data pada kelas mayoritas.\n",
    "#### Catatan: Dapat menyebabkan kehilangan informasi dan memerlukan pemilihan parameter yang hati-hati.\n",
    "### Hybrid Methods: SMOTE-ENN, SMOTE-Tomek Links.\n",
    "\n",
    "#### Kegunaan: Menggabungkan teknik oversampling dan undersampling.\n",
    "#### Catatan: Menyediakan pendekatan seimbang untuk menangani data ketidakseimbangan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Using Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Inisialisasi ADASYN\n",
    "adasyn = ADASYN(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Inisialisasi RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Inisialisasi TomekLinks\n",
    "tomek = TomekLinks(sampling_strategy='auto')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = tomek.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Edited Nearest Neighbors (ENN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "# Inisialisasi EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = enn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYBRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. SMOTE-ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Inisialisasi SMOTEENN\n",
    "smote_enn = SMOTEENN(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. SMOTE-Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Inisialisasi SMOTETomek\n",
    "smote_tomek = SMOTETomek(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. saving and Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untuk data kecil hingga menengah: CSV, Excel, dan JSON.\n",
    "#### Untuk data besar dan efisien: Parquet, Feather, dan HDF5.\n",
    "#### Untuk penyimpanan objek Python: Pickle dan Joblib.\n",
    "#### Untuk mendukung kueri SQL: SQLite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. CSV (Comma-Separated Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke file Excel\n",
    "df.to_excel('preprocessed_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. JSON (JavaScript Object Notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke JSON\n",
    "df.to_json('preprocessed_data.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. SQLite (Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Koneksi ke database SQLite\n",
    "conn = sqlite3.connect('preprocessed_data.db')\n",
    "\n",
    "# Menyimpan dataframe ke database\n",
    "df.to_sql('table_name', conn, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke dalam file pickle\n",
    "df.to_pickle('preprocessed_data.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. HDF5 (Hierarchical Data Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke HDF5\n",
    "df.to_hdf('preprocessed_data.h5', key='df', mode='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke feather\n",
    "df.to_feather('preprocessed_data.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke Parquet\n",
    "df.to_parquet('preprocessed_data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Menyimpan dataframe ke file joblib\n",
    "joblib.dump(df, 'preprocessed_data.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
