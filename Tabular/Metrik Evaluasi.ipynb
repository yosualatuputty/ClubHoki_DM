{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112e1527-2654-4690-af13-071e5e7e912d",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d60f8-b3f3-4d3a-a63b-74bcc62ea7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Menghitung metrik evaluasi\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# ROC AUC\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5f444-6eee-4fe3-a282-d445b0b8b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi Confusion Matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Garis diagonal untuk acuan\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27400f5c-8e19-48be-8253-daf6947069eb",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a206e8f-195b-4833-ba9e-4ca40d010e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Menghitung metrik evaluasi\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"RÂ² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e9654-48a0-4543-85f7-2185829a4467",
   "metadata": {},
   "source": [
    "# Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c89791-a7ec-4b2e-b30a-2ff38d3c8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Terapkan Apriori untuk menemukan itemset yang sering\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "\n",
    "# Terapkan association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Tampilkan aturan asosiasi dan metrik evaluasi\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'leverage', 'conviction']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63f12a-9ffb-43db-9b7a-b473a0b41905",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c87c03-9e88-42c5-9090-2b782aef9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Step 1: Train the model\n",
    "clustering = AgglomerativeClustering(n_clusters=3)\n",
    "labels = clustering.fit_predict(X)\n",
    "\n",
    "# Step 2: Evaluate the model using various metrics\n",
    "\n",
    "# a. Silhouette Score (ranges from -1 to 1, higher is better)\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# b. Calinski-Harabasz Index (higher is better)\n",
    "calinski_harabasz = calinski_harabasz_score(X, labels)\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
    "\n",
    "# c. Davies-Bouldin Index (lower is better)\n",
    "davies_bouldin = davies_bouldin_score(X, labels)\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a227a21b-3b57-4a0f-9b01-ae84a7680c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBScan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Train the DBSCAN model\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Step 2: Filter out the noise points for evaluation (optional, for some metrics)\n",
    "core_samples_mask = labels != -1  # True for core points, False for noise points\n",
    "\n",
    "# Step 3: Evaluate the model using various metrics\n",
    "\n",
    "# a. Silhouette Score (excluding noise points)\n",
    "if len(np.unique(labels[core_samples_mask])) > 1:  # Silhouette score requires > 1 cluster\n",
    "    silhouette_avg = silhouette_score(X[core_samples_mask], labels[core_samples_mask])\n",
    "    print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "else:\n",
    "    print(\"Silhouette Score: Not applicable, only one cluster or all points are noise.\")\n",
    "\n",
    "# b. Calinski-Harabasz Index (works best without noise)\n",
    "if len(np.unique(labels[core_samples_mask])) > 1:\n",
    "    calinski_harabasz = calinski_harabasz_score(X[core_samples_mask], labels[core_samples_mask])\n",
    "    print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
    "else:\n",
    "    print(\"Calinski-Harabasz Index: Not applicable, only one cluster or all points are noise.\")\n",
    "\n",
    "# c. Davies-Bouldin Index (includes noise points)\n",
    "if len(np.unique(labels)) > 1:  # DB index works with noise but requires more than one cluster\n",
    "    davies_bouldin = davies_bouldin_score(X[core_samples_mask], labels[core_samples_mask])\n",
    "    print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
    "else:\n",
    "    print(\"Davies-Bouldin Index: Not applicable, only one cluster or all points are noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5a7e3-2b9b-48ef-a7c0-c129ba5f7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For 2D data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ad127-a3a5-4c6b-a423-cafda5244188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fad40ba-de4c-4e50-b62e-9312dffad1f7",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb830f5-6b17-47b9-877d-096c7a92a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Class SVM\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Train One-Class SVM\n",
    "# X_train contains only the \"inliers\" (normal data)\n",
    "oc_svm = OneClassSVM(kernel='rbf', gamma=0.1, nu=0.1)  # RBF kernel, nu defines the proportion of outliers\n",
    "oc_svm.fit(X_train)  # Train the model on inliers\n",
    "\n",
    "# Step 2: Predict\n",
    "# X_test contains both inliers and outliers (test data)\n",
    "y_pred = oc_svm.predict(X_test)  # Predict the class of test data\n",
    "# One-Class SVM predicts:\n",
    "# - `1` for inliers (normal points)\n",
    "# - `-1` for outliers\n",
    "\n",
    "# Step 3: Evaluation\n",
    "# For labeled test data, true labels (1 for inliers, -1 for outliers)\n",
    "# y_true = np.array([...])  # Ground truth (1 for inliers, -1 for outliers)\n",
    "\n",
    "# a. Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# b. Precision (for detecting outliers)\n",
    "precision = precision_score(y_true, y_pred, pos_label=-1)  # Pos_label=-1 because we are interested in outliers\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# c. Recall (for detecting outliers)\n",
    "recall = recall_score(y_true, y_pred, pos_label=-1)  # Pos_label=-1 for outliers\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# d. ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "# e. Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752635ba-7ea8-4f7c-98c0-801d3aa99292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolation Forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Train Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "anomalies = iso_forest.fit_predict(X)\n",
    "\n",
    "# Step 2: Evaluate the Model\n",
    "# Assuming we have ground truth labels (1 for inliers, -1 for outliers)\n",
    "# y_true = np.array([...])  # Ground truth labels, where 1 = inlier and -1 = outlier\n",
    "\n",
    "# a. Accuracy\n",
    "accuracy = accuracy_score(y_true, anomalies)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# b. Precision (for detecting outliers)\n",
    "precision = precision_score(y_true, anomalies, pos_label=-1)  # Outliers are -1\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# c. Recall (for detecting outliers)\n",
    "recall = recall_score(y_true, anomalies, pos_label=-1)  # Outliers are -1\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# d. ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_true, anomalies)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "# e. Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, anomalies)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7097c31-3354-412b-bc0c-de4c6d338628",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf17f37-9acf-4ba2-acc0-61d6f05c5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Train PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "X_reduced = pca.fit_transform(X)  # Fit PCA and reduce the dimensionality of X\n",
    "\n",
    "# Step 2: Evaluation\n",
    "\n",
    "# a. Explained Variance Ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio: {explained_variance}\")\n",
    "\n",
    "# b. Total Variance Explained (cumulative sum)\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "print(f\"Cumulative Explained Variance: {cumulative_explained_variance}\")\n",
    "\n",
    "# c. Reconstruction Error (Optional)\n",
    "# To check how well the reduced data reconstructs the original data\n",
    "X_reconstructed = pca.inverse_transform(X_reduced)\n",
    "reconstruction_error = np.mean((X - X_reconstructed) ** 2)\n",
    "print(f\"Reconstruction Error: {reconstruction_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d85e38-9142-444e-92e0-9e48df1dea3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_latihan",
   "language": "python",
   "name": "env_latihan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
