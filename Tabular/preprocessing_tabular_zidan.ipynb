{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "import pycaret\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # For scaling\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder  # For encoding categorical data\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Lokal: CSV, Excel, JSON, ZIP.\n",
    "#### Database: SQL.\n",
    "#### Big Data: Parquet, Feather, HDF5.\n",
    "#### Online Resources: URL, API, Google Sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agar Col Ditampilkan Semua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atur tampilan agar semua kolom ditampilkan\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Contoh data\n",
    "data = {'col1': range(10), 'col2': range(10, 20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('your_dataset.csv')\n",
    "df.head()  # Preview the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from an Excel file\n",
    "df = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a JSON file\n",
    "df = pd.read_json('data.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('database.db')\n",
    "\n",
    "# Load data from an SQL query\n",
    "df = pd.read_sql_query('SELECT * FROM tablename', conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a compressed ZIP file\n",
    "df = pd.read_csv('data.zip', compression='zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a URL\n",
    "df = pd.read_csv('https://url_to_data.com/data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Google Sheets (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Authenticate using Google API\n",
    "scope = ['https://spreadsheets.google.com/feeds']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open a sheet by name\n",
    "sheet = client.open('SheetName').sheet1\n",
    "\n",
    "# Get data as a list of lists\n",
    "data = sheet.get_all_records()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. API (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load data from a REST API\n",
    "response = requests.get('https://api.example.com/data')\n",
    "data = response.json()\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Parquet (HDFS/S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Parquet file\n",
    "df = pd.read_parquet('data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a Feather file\n",
    "df = pd.read_feather('data.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K. HDF5 (Hierarchical Data Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from an HDF5 file\n",
    "df = pd.read_hdf('data.h5', 'key_name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L. Pickle (Python Serializing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a Pickle file\n",
    "df = pd.read_pickle('data.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M. arff (Attribute-Relation File Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff('data.arff')\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penghapusan Baris/Kolom: Jika nilai hilang minor.\n",
    "#### Pengisian Nilai: Menggunakan konstanta, statistik deskriptif, interpolasi, atau model.\n",
    "#### Model Pembelajaran Mesin: Menggunakan KNN, regresi, atau algoritma khusus untuk mengisi nilai hilang.\n",
    "#### Khusus Kategori atau Data Waktu: Mengisi berdasarkan kategori atau metode forward/backward fill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Detect Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()  # Check how many missing values are in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete Row with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Menghapus baris dengan nilai hilang\n",
    "df = df.dropna()\n",
    "\n",
    "# Menghapus kolom dengan nilai hilang\n",
    "df = df.dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Fill Missing Values (Constanta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dengan nol\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Mengisi nilai hilang dengan nilai konstan lain\n",
    "df = df.fillna(value={'column1': 0, 'column2': 'unknown'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Fill Missing Values (Mean, Median, Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dengan rata-rata kolom\n",
    "df['column'] = df['column'].fillna(df['column'].mean())\n",
    "\n",
    "# Mengisi nilai hilang dengan median kolom\n",
    "df['column'] = df['column'].fillna(df['column'].median())\n",
    "\n",
    "# Mengisi nilai hilang dengan modus kolom\n",
    "df['column'] = df['column'].fillna(df['column'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Fill Missing Values (Category Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dengan modus kategori\n",
    "df['column'] = df.groupby('category')['column'].transform(lambda x: x.fillna(x.mode()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Fill Missing Values (Pembobotan dan Penyesuaian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dalam data waktu dengan pembobotan\n",
    "df['column'] = df['column'].fillna(method='ffill')  # Forward fill\n",
    "df['column'] = df['column'].fillna(method='bfill')  # Backward fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Fill Missing Values (Interpolasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi nilai hilang dengan interpolasi linear\n",
    "df = df.interpolate()\n",
    "\n",
    "# Mengisi nilai hilang dengan interpolasi polinomial\n",
    "df = df.interpolate(method='polynomial', order=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Fill Missing Values (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Menggunakan KNN untuk imputasi\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Fill Missing Values (Regression Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Memisahkan data dengan nilai hilang\n",
    "df_missing = df[df['column'].isna()]\n",
    "df_complete = df.dropna(subset=['column'])\n",
    "\n",
    "# Melatih model regresi\n",
    "X = df_complete.drop(columns=['column'])\n",
    "y = df_complete['column']\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Mengisi nilai hilang dengan prediksi model\n",
    "X_missing = df_missing.drop(columns=['column'])\n",
    "df_missing['column'] = model.predict(X_missing)\n",
    "df = pd.concat([df_complete, df_missing])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K. Fill Missing Values (IterativeImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Menggunakan IterativeImputer\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L. Fill Missing Values (Imputation Model Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numerical data with the median\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df['num_column'] = num_imputer.fit_transform(df[['num_column']])\n",
    "\n",
    "# Fill missing categorical data with the most frequent value\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['cat_column'] = cat_imputer.fit_transform(df[['cat_column']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M. Cetak Tabel Missing Value dengan Presentase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding: Digunakan untuk data kategorikal tanpa urutan.\n",
    "#### One-Hot Encoding: Untuk model yang tidak mendukung data kategorikal.\n",
    "#### Binary Encoding dan BaseN Encoding: Untuk mengurangi dimensi dibandingkan one-hot encoding.\n",
    "#### Frequency dan Count Encoding: Menggunakan frekuensi kategori.\n",
    "#### Target Encoding: Menggunakan rata-rata target per kategori.\n",
    "#### Ordinal Encoding: Untuk data kategorikal yang memiliki urutan.\n",
    "#### Hash Encoding dan Leave-One-Out Encoding: Untuk data besar atau data dengan banyak kategori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Label Encoding (For Ordinal Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['category_encoded'] = label_encoder.fit_transform(df['category_column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. One-Hot Encoding (For Nominal Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['category_column'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "# Inisialisasi BinaryEncoder\n",
    "encoder = ce.BinaryEncoder(cols=['categorical_column'])\n",
    "\n",
    "# Menggunakan BinaryEncoder\n",
    "df_encoded = encoder.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung frekuensi\n",
    "frequency = df['categorical_column'].value_counts()\n",
    "\n",
    "# Mengganti kategori dengan frekuensi\n",
    "df['encoded_column'] = df['categorical_column'].map(frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Target Encoding (Mean Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung rata-rata target per kategori\n",
    "mean_encoding = df.groupby('categorical_column')['target'].mean()\n",
    "\n",
    "# Mengganti kategori dengan rata-rata target\n",
    "df['encoded_column'] = df['categorical_column'].map(mean_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Inisialisasi OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Menggunakan OrdinalEncoder pada kolom\n",
    "df['encoded_column'] = encoder.fit_transform(df[['categorical_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. BaseN Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Inisialisasi BaseNEncoder dengan basis 3\n",
    "encoder = ce.BaseNEncoder(base=3, cols=['categorical_column'])\n",
    "\n",
    "# Menggunakan BaseNEncoder\n",
    "df_encoded = encoder.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Count Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung jumlah kemunculan\n",
    "count_encoding = df['categorical_column'].value_counts()\n",
    "\n",
    "# Mengganti kategori dengan jumlah kemunculan\n",
    "df['encoded_column'] = df['categorical_column'].map(count_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Hash Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Inisialisasi HashingEncoder dengan n_components\n",
    "encoder = ce.HashingEncoder(n_components=8, cols=['categorical_column'])\n",
    "\n",
    "# Menggunakan HashingEncoder\n",
    "df_encoded = encoder.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Leave-One-Out Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Inisialisasi HashingEncoder dengan n_components\n",
    "encoder = ce.HashingEncoder(n_components=8, cols=['categorical_column'])\n",
    "\n",
    "# Menggunakan HashingEncoder\n",
    "df_encoded = encoder.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization: Menyediakan fitur dengan distribusi normal.\n",
    "#### Min-Max Scaling: Mengubah rentang data ke [0, 1].\n",
    "#### MaxAbs Scaling: Berguna untuk data dengan nilai positif dan negatif.\n",
    "#### Robust Scaling: Mengurangi dampak outlier.\n",
    "#### Quantile Transformation: Mengubah distribusi fitur.\n",
    "#### Power Transformation: Membantu stabilisasi varians.\n",
    "#### Normalizer: Menormalkan fitur ke norma unit.\n",
    "#### Custom Scaling: Menyediakan kontrol penuh untuk scaling.\n",
    "#### Binarization: Mengubah fitur ke format biner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Standardization (Z-score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['num_column1', 'num_column2']] = scaler.fit_transform(df[['num_column1', 'num_column2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "df[['num_column1', 'num_column2']] = min_max_scaler.fit_transform(df[['num_column1', 'num_column2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. MaxAbs Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Inisialisasi MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Robust Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Inisialisasi RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Quantile Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Inisialisasi QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Power Transformation (Box-Cox & Yeo-Johnson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Inisialisasi PowerTransformer dengan metode Yeo-Johnson\n",
    "scaler = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Inisialisasi Normalizer dengan norma L2\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Unit Vector Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Inisialisasi Normalizer dengan norma L2\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Custom Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fungsi custom untuk scaling\n",
    "def custom_scaling(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "# Menerapkan custom scaling\n",
    "df['scaled_column'] = custom_scaling(df['feature_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K. Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Inisialisasi Binarizer dengan threshold\n",
    "scaler = Binarizer(threshold=0.5)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "scaled_data = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Splitting Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation: Menggunakan seluruh data untuk pelatihan dan pengujian, memberikan penilaian model yang lebih robust.\n",
    "#### Stratified K-Fold: Menjaga distribusi target dalam setiap fold.\n",
    "#### Time Series Split: Menjaga urutan waktu dalam data.\n",
    "#### LOOCV: Membagi data dengan satu data sebagai uji pada setiap iterasi.\n",
    "#### Group K-Fold: Mempertahankan kelompok tertentu dalam data.\n",
    "#### Shuffle Split: Membagi data secara acak dengan proporsi yang ditentukan.\n",
    "#### Custom Split: Pembagian data berdasarkan kriteria khusus.\n",
    "#### Bootstrap Sampling: Menggunakan sampling dengan pengembalian untuk data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Split Standar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Inisialisasi KFold dengan jumlah fold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Menggunakan KFold untuk membagi data\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Stratified K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Inisialisasi StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Menggunakan StratifiedKFold untuk membagi data\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Inisialisasi TimeSeriesSplit dengan jumlah fold\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Menggunakan TimeSeriesSplit untuk membagi data\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Leave-One-Out Cross-Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Inisialisasi LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Menggunakan LeaveOneOut untuk membagi data\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Group K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Inisialisasi GroupKFold dengan jumlah fold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Menggunakan GroupKFold untuk membagi data\n",
    "for train_index, test_index in gkf.split(X, y, groups=groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Shuffle Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Inisialisasi ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "# Menggunakan ShuffleSplit untuk membagi data\n",
    "for train_index, test_index in ss.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh custom split berdasarkan kriteria waktu\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Menggunakan resample untuk membuat subset\n",
    "X_train, X_test, y_train, y_test = resample(X, y, n_samples=0.8, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifikasi dan Penghapusan: Menggunakan metode statistik atau visual untuk mengidentifikasi dan menghapus outliers.\n",
    "#### Transformasi Data: Menggunakan transformasi matematis untuk mengurangi efek outliers.\n",
    "#### Robust Methods: Metode scaling dan winsorizing yang mengurangi dampak outliers.\n",
    "#### Imputasi Outliers: Mengganti atau interpolasi nilai outliers untuk menjaga konsistensi data.\n",
    "#### Algorithm-Based Methods: Menggunakan algoritma machine learning untuk mendeteksi dan menangani outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Using Z-score to Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "z_scores = np.abs(stats.zscore(df[['num_column1', 'num_column2']]))\n",
    "df_cleaned = df[(z_scores < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. IQR (Interquartile Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['feature_column'].quantile(0.25)\n",
    "Q3 = df['feature_column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Menentukan batas bawah dan atas\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "# Mengidentifikasi outliers\n",
    "outliers = (df['feature_column'] < lower_bound) | (df['feature_column'] > upper_bound)\n",
    "# Menghapus outliers\n",
    "df_no_outliers = df[~outliers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Visualisasi (Boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Membuat boxplot\n",
    "sns.boxplot(x=df['feature_column'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mengaplikasikan log transformation\n",
    "df['log_feature'] = np.log1p(df['feature_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Square Root Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengaplikasikan square root transformation\n",
    "df['sqrt_feature'] = np.sqrt(df['feature_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Robust Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Inisialisasi RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit dan transformasi data\n",
    "df[['scaled_feature']] = scaler.fit_transform(df[['feature_column']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Trimming/ Winsorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats\n",
    "\n",
    "# Winsorizing data\n",
    "df['winsorized_feature'] = mstats.winsorize(df['feature_column'], limits=[0.05, 0.05])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Imputasi dengan Nilai Median atau Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengganti outliers dengan median\n",
    "median_value = df['feature_column'].median()\n",
    "df['feature_column'] = np.where(outliers, median_value, df['feature_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Imputasi dengan Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolasi data\n",
    "df['interpolated_feature'] = df['feature_column'].interpolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Inisialisasi IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.01)\n",
    "\n",
    "# Fit model\n",
    "outliers = iso_forest.fit_predict(df[['feature_column']])\n",
    "# Mengidentifikasi outliers\n",
    "df_no_outliers = df[outliers == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K. One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Inisialisasi OneClassSVM\n",
    "ocsvm = OneClassSVM(gamma='auto', nu=0.01)\n",
    "\n",
    "# Fit model\n",
    "outliers = ocsvm.fit_predict(df[['feature_column']])\n",
    "# Mengidentifikasi outliers\n",
    "df_no_outliers = df[outliers == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-Based Feature Importance: Menggunakan output model untuk menilai pentingnya fitur.\n",
    "#### Recursive Feature Elimination (RFE): Metode iteratif untuk menghapus fitur yang tidak penting.\n",
    "#### Mutual Information: Mengukur ketergantungan antara fitur dan target.\n",
    "#### Correlation Matrix: Mengidentifikasi fitur yang berkorelasi dengan target atau antar fitur.\n",
    "#### L1 Regularization (Lasso): Menggunakan regularisasi untuk memilih fitur penting.\n",
    "#### Genetic Algorithms: Metode berbasis evolusi untuk memilih fitur optimal.\n",
    "#### Feature Selection using AutoML: Mengotomatisasi proses feature selection dan tuning.\n",
    "#### Unsupervised Feature Selection: Menggunakan teknik unsupervised untuk memilih fitur penting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Removing Low Variance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "df_high_variance = selector.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Model-Based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inisialisasi dan fit model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Mengurutkan fitur berdasarkan kepentingan\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in indices:\n",
    "    print(f\"Feature {i}: {importances[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Menghitung permutation feature importance\n",
    "results = permutation_importance(model, X_test, y_test, scoring='accuracy')\n",
    "importances = results.importances_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inisialisasi model dan RFE\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit RFE\n",
    "fit = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan fitur yang terpilih\n",
    "selected_features = fit.support_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Menghitung mutual information\n",
    "mi = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "# Mengurutkan fitur berdasarkan mutual information\n",
    "indices = np.argsort(mi)[::-1]\n",
    "for i in indices:\n",
    "    print(f\"Feature {i}: {mi[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. L1 Regularization (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Inisialisasi Lasso\n",
    "lasso = Lasso(alpha=0.01)\n",
    "\n",
    "# Fit model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan koefisien fitur\n",
    "coefficients = lasso.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Inisialisasi RFECV dengan Logistic Regression\n",
    "selector = RFECV(LogisticRegression(), step=1, cv=5)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan fitur yang terpilih\n",
    "selected_features = selector.support_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Feature Selection using AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Inisialisasi TPOT\n",
    "tpot = TPOTClassifier(verbosity=2)\n",
    "\n",
    "# Fit model\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Mendapatkan pipeline terbaik\n",
    "print(tpot.fitted_pipeline_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Unsupervised Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Inisialisasi PCA\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# Fit PCA\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Mendapatkan komponen utama\n",
    "components = pca.components_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Binning Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal Width Binning: Membagi rentang data menjadi bin dengan lebar yang sama.\n",
    "#### Equal Frequency Binning: Membagi data sehingga setiap bin memiliki jumlah data yang sama.\n",
    "#### Custom Binning: Menetapkan interval bin secara manual.\n",
    "#### K-Means Binning: Menggunakan clustering untuk menentukan bin.\n",
    "#### Decision Tree Binning: Menentukan bin berdasarkan model decision tree.\n",
    "#### Equal Width Binning with Variable Bin Width: Menggunakan bin dengan lebar bervariasi berdasarkan distribusi data.\n",
    "#### Binning with Aggregation: Menghitung statistik agregasi per bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Equal Width Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Mendefinisikan jumlah bin\n",
    "num_bins = 5\n",
    "\n",
    "# Menggunakan cut untuk binning\n",
    "df['binned_feature'] = pd.cut(df['feature'], bins=num_bins, labels=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Equal Frequency Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Mendefinisikan jumlah bin\n",
    "num_bins = 5\n",
    "\n",
    "# Menggunakan qcut untuk binning\n",
    "df['binned_feature'] = pd.qcut(df['feature'], q=num_bins, labels=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Custom Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Mendefinisikan interval bin\n",
    "bins = [-np.inf, -1, 0, 1, np.inf]\n",
    "labels = ['Very Low', 'Low', 'High', 'Very High']\n",
    "\n",
    "# Menggunakan cut untuk binning\n",
    "df['binned_feature'] = pd.cut(df['feature'], bins=bins, labels=labels)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. K-Means Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Menentukan jumlah cluster (bin)\n",
    "num_bins = 5\n",
    "\n",
    "# Inisialisasi dan fit KMeans\n",
    "kmeans = KMeans(n_clusters=num_bins, random_state=0).fit(df[['feature']])\n",
    "\n",
    "# Menambahkan label cluster ke dataframe\n",
    "df['binned_feature'] = kmeans.labels_\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Decision Tree Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100), 'target': np.random.randn(100)})\n",
    "\n",
    "# Inisialisasi dan fit DecisionTreeRegressor\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=5, random_state=0)\n",
    "tree.fit(df[['feature']], df['target'])\n",
    "\n",
    "# Mendapatkan batas bin\n",
    "bins = np.sort(tree.tree_.threshold[tree.tree_.threshold != -2])\n",
    "bins = np.append(bins, np.inf)  # Menambahkan batas atas\n",
    "\n",
    "# Menggunakan cut untuk binning\n",
    "df['binned_feature'] = pd.cut(df['feature'], bins=bins, include_lowest=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Equal Width Binning with Variable Bin Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Mendefinisikan lebar bin berdasarkan persentil\n",
    "percentiles = [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "bins = np.percentile(df['feature'], q=percentiles)\n",
    "\n",
    "# Menggunakan cut untuk binning\n",
    "df['binned_feature'] = pd.cut(df['feature'], bins=bins, labels=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Binning with Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contoh data\n",
    "df = pd.DataFrame({'feature': np.random.randn(100)})\n",
    "\n",
    "# Menggunakan qcut untuk binning\n",
    "df['binned_feature'] = pd.qcut(df['feature'], q=5, labels=False)\n",
    "\n",
    "# Menghitung statistik agregasi per bin\n",
    "bin_stats = df.groupby('binned_feature').agg({'feature': ['mean', 'std', 'count']})\n",
    "\n",
    "print(bin_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Data Balancing (Handling Imbalanced Classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling Methods: SMOTE, ADASYN, Random Oversampling.\n",
    "\n",
    "#### Kegunaan: Menambah data pada kelas minoritas.\n",
    "#### Catatan: Memerlukan penanganan overfitting dan mungkin memperbesar ukuran dataset.\n",
    "### Undersampling Methods: Random Undersampling, Tomek Links, Edited Nearest Neighbors.\n",
    "\n",
    "#### Kegunaan: Mengurangi data pada kelas mayoritas.\n",
    "#### Catatan: Dapat menyebabkan kehilangan informasi dan memerlukan pemilihan parameter yang hati-hati.\n",
    "### Hybrid Methods: SMOTE-ENN, SMOTE-Tomek Links.\n",
    "\n",
    "#### Kegunaan: Menggabungkan teknik oversampling dan undersampling.\n",
    "#### Catatan: Menyediakan pendekatan seimbang untuk menangani data ketidakseimbangan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Using Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Inisialisasi ADASYN\n",
    "adasyn = ADASYN(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Inisialisasi RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Inisialisasi TomekLinks\n",
    "tomek = TomekLinks(sampling_strategy='auto')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = tomek.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Edited Nearest Neighbors (ENN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "# Inisialisasi EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = enn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYBRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. SMOTE-ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Inisialisasi SMOTEENN\n",
    "smote_enn = SMOTEENN(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. SMOTE-Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Inisialisasi SMOTETomek\n",
    "smote_tomek = SMOTETomek(sampling_strategy='auto', random_state=0)\n",
    "\n",
    "# Fit dan transformasi data\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. saving and Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untuk data kecil hingga menengah: CSV, Excel, dan JSON.\n",
    "#### Untuk data besar dan efisien: Parquet, Feather, dan HDF5.\n",
    "#### Untuk penyimpanan objek Python: Pickle dan Joblib.\n",
    "#### Untuk mendukung kueri SQL: SQLite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. CSV (Comma-Separated Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke file Excel\n",
    "df.to_excel('preprocessed_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. JSON (JavaScript Object Notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke JSON\n",
    "df.to_json('preprocessed_data.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. SQLite (Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Koneksi ke database SQLite\n",
    "conn = sqlite3.connect('preprocessed_data.db')\n",
    "\n",
    "# Menyimpan dataframe ke database\n",
    "df.to_sql('table_name', conn, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke dalam file pickle\n",
    "df.to_pickle('preprocessed_data.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. HDF5 (Hierarchical Data Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke HDF5\n",
    "df.to_hdf('preprocessed_data.h5', key='df', mode='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke feather\n",
    "df.to_feather('preprocessed_data.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataframe ke Parquet\n",
    "df.to_parquet('preprocessed_data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Menyimpan dataframe ke file joblib\n",
    "joblib.dump(df, 'preprocessed_data.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tambahan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyh311",
   "language": "python",
   "name": "pyh311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
